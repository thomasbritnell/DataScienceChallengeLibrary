{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomasbritnell/DataScienceChallengeLibrary/blob/main/dataset_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "582a44ab",
      "metadata": {
        "id": "582a44ab"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q pandas numpy scikit-learn openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aaa55bdb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaa55bdb",
        "outputId": "48d89ff6-174b-4f24-a693-b9cf2e05e0d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "96f03b40",
      "metadata": {
        "id": "96f03b40"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import os, zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set paths\n",
        "base_path = '/content/drive/MyDrive/AI_Datasets'\n",
        "cleaned_path = os.path.join(base_path, 'cleaned_datasets')\n",
        "os.makedirs(cleaned_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e60a88b2",
      "metadata": {
        "id": "e60a88b2"
      },
      "source": [
        "### Unzip `AmazonReviews.zip`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "58b59d1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58b59d1e",
        "outputId": "70a30db4-4f11-4502-a79c-92c0faf07c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipped: AmazonReviews.zip\n"
          ]
        }
      ],
      "source": [
        "with zipfile.ZipFile(os.path.join(base_path, 'AmazonReviews.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(base_path)\n",
        "print('Unzipped: AmazonReviews.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0802334e",
      "metadata": {
        "id": "0802334e"
      },
      "source": [
        "### Unzip `creditcard.csv.zip`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d82d666d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d82d666d",
        "outputId": "9e13f603-206c-4daa-daea-c01fb478d77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipped: creditcard.csv.zip\n"
          ]
        }
      ],
      "source": [
        "with zipfile.ZipFile(os.path.join(base_path, 'creditcard.csv.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(base_path)\n",
        "print('Unzipped: creditcard.csv.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5391ae9",
      "metadata": {
        "id": "a5391ae9"
      },
      "source": [
        "### Unzip `dirty_deputies.csv.zip`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2f7f0ebc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f7f0ebc",
        "outputId": "89248f46-b3eb-40f8-85d5-798037600c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipped: dirty_deputies.csv.zip\n"
          ]
        }
      ],
      "source": [
        "with zipfile.ZipFile(os.path.join(base_path, 'dirty_deputies.csv.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(base_path)\n",
        "print('Unzipped: dirty_deputies.csv.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c5b94f",
      "metadata": {
        "id": "c8c5b94f"
      },
      "source": [
        "### Preprocess `Cleveland.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cf9b8d63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf9b8d63",
        "outputId": "656be7fe-9785-437b-ace4-e5bc857a18bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (297, 14)\n",
            "Saved cleaned dataset: cleaned_Cleveland.csv\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess Cleveland.csv\n",
        "file_path = os.path.join(base_path, 'Cleveland.csv')\n",
        "df = pd.read_csv(file_path)\n",
        "print('Original shape:', df.shape)\n",
        "\n",
        "# Drop columns with >50% missing\n",
        "df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
        "\n",
        "# Identify column types\n",
        "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
        "categorical_cols = df.select_dtypes(exclude='number').columns.tolist()\n",
        "\n",
        "# Impute\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = SimpleImputer(strategy='mean').fit_transform(df[numeric_cols])\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'missing')\n",
        "\n",
        "# Encode categoricals\n",
        "for col in categorical_cols:\n",
        "    if df[col].nunique() <= 10:\n",
        "        df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "    else:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Scale numeric columns\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])\n",
        "\n",
        "# Save cleaned version\n",
        "df.to_csv(os.path.join(cleaned_path, 'cleaned_Cleveland.csv'), index=False)\n",
        "print('Saved cleaned dataset: cleaned_Cleveland.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c409d8b5",
      "metadata": {
        "id": "c409d8b5"
      },
      "source": [
        "### Preprocess `netflix_titles.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b999b812",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b999b812",
        "outputId": "1e1af45d-254d-4b59-f907-1cd47d4f3f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (8807, 12)\n",
            "Saved cleaned dataset: cleaned_netflix_titles.csv\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess netflix_titles.csv\n",
        "file_path = os.path.join(base_path, 'netflix_titles.csv')\n",
        "df = pd.read_csv(file_path)\n",
        "print('Original shape:', df.shape)\n",
        "\n",
        "# Drop columns with >50% missing\n",
        "df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
        "\n",
        "# Identify column types\n",
        "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
        "categorical_cols = df.select_dtypes(exclude='number').columns.tolist()\n",
        "\n",
        "# Impute\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = SimpleImputer(strategy='mean').fit_transform(df[numeric_cols])\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'missing')\n",
        "\n",
        "# Encode categoricals\n",
        "for col in categorical_cols:\n",
        "    if df[col].nunique() <= 10:\n",
        "        df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "    else:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Scale numeric columns\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])\n",
        "\n",
        "# Save cleaned version\n",
        "df.to_csv(os.path.join(cleaned_path, 'cleaned_netflix_titles.csv'), index=False)\n",
        "print('Saved cleaned dataset: cleaned_netflix_titles.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5c62a1",
      "metadata": {
        "id": "7d5c62a1"
      },
      "source": [
        "### Preprocess `SpotifyFeatures.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2110cc90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2110cc90",
        "outputId": "81294e0f-b4e1-45c8-cc98-0c20edd60e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (232725, 18)\n",
            "Saved cleaned dataset: cleaned_SpotifyFeatures.csv\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess SpotifyFeatures.csv\n",
        "file_path = os.path.join(base_path, 'SpotifyFeatures.csv')\n",
        "df = pd.read_csv(file_path)\n",
        "print('Original shape:', df.shape)\n",
        "\n",
        "# Drop columns with >50% missing\n",
        "df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
        "\n",
        "# Identify column types\n",
        "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
        "categorical_cols = df.select_dtypes(exclude='number').columns.tolist()\n",
        "\n",
        "# Impute\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = SimpleImputer(strategy='mean').fit_transform(df[numeric_cols])\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'missing')\n",
        "\n",
        "# Encode categoricals\n",
        "for col in categorical_cols:\n",
        "    if df[col].nunique() <= 10:\n",
        "        df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "    else:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Scale numeric columns\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])\n",
        "\n",
        "# Save cleaned version\n",
        "df.to_csv(os.path.join(cleaned_path, 'cleaned_SpotifyFeatures.csv'), index=False)\n",
        "print('Saved cleaned dataset: cleaned_SpotifyFeatures.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07be6db9",
      "metadata": {
        "id": "07be6db9"
      },
      "source": [
        "### Preprocess `Titanic.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "147c6b70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "147c6b70",
        "outputId": "dafcc93d-a16b-4454-b339-af894efc57bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (1309, 28)\n",
            "Saved cleaned dataset: cleaned_Titanic.csv\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess Titanic.csv\n",
        "file_path = os.path.join(base_path, 'Titanic.csv')\n",
        "df = pd.read_csv(file_path)\n",
        "print('Original shape:', df.shape)\n",
        "\n",
        "# Drop columns with >50% missing\n",
        "df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
        "\n",
        "# Identify column types\n",
        "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
        "categorical_cols = df.select_dtypes(exclude='number').columns.tolist()\n",
        "\n",
        "# Impute\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = SimpleImputer(strategy='mean').fit_transform(df[numeric_cols])\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'missing')\n",
        "\n",
        "# Encode categoricals\n",
        "for col in categorical_cols:\n",
        "    if df[col].nunique() <= 10:\n",
        "        df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "    else:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Scale numeric columns\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])\n",
        "\n",
        "# Save cleaned version\n",
        "df.to_csv(os.path.join(cleaned_path, 'cleaned_Titanic.csv'), index=False)\n",
        "print('Saved cleaned dataset: cleaned_Titanic.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c967af5",
      "metadata": {
        "id": "0c967af5"
      },
      "source": [
        "### Preprocess `AmazonReviews.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a9a15c3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9a15c3e",
        "outputId": "11d91cdf-a581-4373-a875-0cc5a525ab56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (568454, 10)\n",
            "Saved cleaned dataset: cleaned_AmazonReviews.csv\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess AmazonReviews.csv\n",
        "file_path = os.path.join(base_path, 'Reviews.csv')\n",
        "df = pd.read_csv(file_path)\n",
        "print('Original shape:', df.shape)\n",
        "\n",
        "# Drop columns with >50% missing\n",
        "df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
        "\n",
        "# Identify column types\n",
        "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
        "categorical_cols = df.select_dtypes(exclude='number').columns.tolist()\n",
        "\n",
        "# Impute\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = SimpleImputer(strategy='mean').fit_transform(df[numeric_cols])\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'missing')\n",
        "\n",
        "# Encode categoricals\n",
        "for col in categorical_cols:\n",
        "    if df[col].nunique() <= 10:\n",
        "        df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "    else:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Scale numeric columns\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])\n",
        "\n",
        "# Save cleaned version\n",
        "df.to_csv(os.path.join(cleaned_path, 'cleaned_AmazonReviews.csv'), index=False)\n",
        "print('Saved cleaned dataset: cleaned_AmazonReviews.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "912f91e4",
      "metadata": {
        "id": "912f91e4"
      },
      "source": [
        "### Preprocess `creditcard.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "70c7868b",
      "metadata": {
        "id": "70c7868b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e3804e7-73f9-4985-c3ec-1e7ec7336fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (284807, 31)\n",
            "Saved cleaned dataset: cleaned_creditcard.csv\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess creditcard.csv\n",
        "file_path = os.path.join(base_path, 'creditcard.csv')\n",
        "df = pd.read_csv(file_path)\n",
        "print('Original shape:', df.shape)\n",
        "\n",
        "# Drop columns with >50% missing\n",
        "df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
        "\n",
        "# Identify column types\n",
        "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
        "categorical_cols = df.select_dtypes(exclude='number').columns.tolist()\n",
        "\n",
        "# Impute\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = SimpleImputer(strategy='mean').fit_transform(df[numeric_cols])\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'missing')\n",
        "\n",
        "# Encode categoricals\n",
        "for col in categorical_cols:\n",
        "    if df[col].nunique() <= 10:\n",
        "        df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "    else:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Scale numeric columns\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])\n",
        "\n",
        "# Save cleaned version\n",
        "df.to_csv(os.path.join(cleaned_path, 'cleaned_creditcard.csv'), index=False)\n",
        "print('Saved cleaned dataset: cleaned_creditcard.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb4bf6fb",
      "metadata": {
        "id": "fb4bf6fb"
      },
      "source": [
        "### Preprocess `dirty_deputies.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5e37469a",
      "metadata": {
        "id": "5e37469a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcdd599-b041-4303-ccfb-c877e36233d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (339089, 18)\n",
            "Saved cleaned dataset: cleaned_dirty_deputies.csv\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess dirty_deputies.csv\n",
        "file_path = os.path.join(base_path, 'dirty_deputies.csv')\n",
        "df = pd.read_csv(file_path)\n",
        "print('Original shape:', df.shape)\n",
        "\n",
        "# Drop columns with >50% missing\n",
        "df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
        "\n",
        "# Identify column types\n",
        "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
        "categorical_cols = df.select_dtypes(exclude='number').columns.tolist()\n",
        "\n",
        "# Impute\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = SimpleImputer(strategy='mean').fit_transform(df[numeric_cols])\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'missing')\n",
        "\n",
        "# Encode categoricals\n",
        "for col in categorical_cols:\n",
        "    if df[col].nunique() <= 10:\n",
        "        df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "    else:\n",
        "        df[col] = df[col].astype(str)\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Scale numeric columns\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])\n",
        "\n",
        "# Save cleaned version\n",
        "df.to_csv(os.path.join(cleaned_path, 'cleaned_dirty_deputies.csv'), index=False)\n",
        "print('Saved cleaned dataset: cleaned_dirty_deputies.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}